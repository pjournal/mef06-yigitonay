[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yigit Onay Progress Journal",
    "section": "",
    "text": "Introduction\n\nThis progress journal covers Yiğit Onay’s work during their term at BDA 503 Fall 2022.\nEach section is an assignment or an individual work."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  BDA-503 Data Analytics Essentials: Assignment 1",
    "section": "",
    "text": "Hi. This is Yigit. I was born in Ankara, in 1990. I obtained my undergraduate degree from Department of Economics at Bogazici University. After my graduation, I passed the three-stage examination process of Turkiye İş Bankası and started as assistant auditor, which gave me a solid background in banking sector such as the business structure of both headquarters, branches, and their subsidiaries, risk analysis of loan portfolios of corporates and SMEs.\nAs an advancement in my career, I passed the researcher examination of The Central Bank of Republic of Turkey (CBRT) Turkey, and started to work as a researcher. Currently, I am working in Markets Department as an assistant economist in CBRT. My fields of interests consist of macroeconomics, financial modelling and empirical asset pricing in general. To be more specific, most of my studies are about pricing of financial derivatives. Some of my papers with my colleagues are The Determinants of Currency Risk Premium in Emerging Market Countries, A Measure of Turkey’s Sovereign and Banking Sector Credit Risk: Asset Swap Spreads.\nAs we live in a world of massive data, being able to use them efficiently is crucial. The processes of collecting data, analysing them, illustrating the results and preparing automated reports as well as being able to apply modern machine learning techniques to macro-financial data are the main accomplishments I want to make at the end of the Big Data Analytics program.\nYou can reach me via my linkedin page."
  },
  {
    "objectID": "assignment1.html#user-2022-videos",
    "href": "assignment1.html#user-2022-videos",
    "title": "1  BDA-503 Data Analytics Essentials: Assignment 1",
    "section": "1.2 UseR-2022 Videos",
    "text": "1.2 UseR-2022 Videos\nThis section summarise the speech given by Devin Pastoor titled Websites & Books & Blogs, oh my! Creating Rich Content with Quarto during RStudio Global 2022 conference. I chose this topic as Quarto is the main tool for scripting our coursework in our course this semester and “Devin Pastorr” decribes the novalties Quarto brings to business and academics.\nThe opportunity to build on a unit of Rmarkdown into more complex content such as websites, books and blogs required additional programs to which lots of effort had been made.In this regard, many different packages/programs had been introduced, such as bookdown or blogdown, which results in the seperation of the comnnutiy.\nDevin Pastoor emphasizes the signficance of Quarto as it\n\nunifies teams across language boundaries and procedural differences and\ngives unparalelled ability to processing and producing the content in various forms."
  },
  {
    "objectID": "assignment1.html#miscallaneous-r-posts",
    "href": "assignment1.html#miscallaneous-r-posts",
    "title": "1  BDA-503 Data Analytics Essentials: Assignment 1",
    "section": "1.3 Miscallaneous R-Posts",
    "text": "1.3 Miscallaneous R-Posts\nIn this section, 3 R-posts are chosen to be outlined.\n\n1.3.1 Difference between R and Python\nThis post discusses the main properties of R and Python and overview their similarities and differences.\n\n\n\n\nFigure-1\n\n\n\nR\n\nR has undergone two decades of development by statisticians and academics.\nIt is the preferred choice for statistical analysis, especially for specialist analytical work, thanks to its extensive library.\n\nPython\n\nPython is a tool for large-scale machine learning deployment and implementation.\nIt is able to perform many of the same activities as R, including data manipulation, engineering, feature selection, web scraping, and app development.\n\nComparing R & Python;\nTheir similarities include - Both the open-source programming languages R and Python have a sizable user base - Their individual catalogs are always being updated with new libraries or tools. - Both can manage very large databases.\nThey differ in certain ways such that: - Python offers a more all-encompassing approach to data science whereas R is primarily employed for statistical analysis. - Python users tend to be programmers and developers, whereas R users are primarily academics and R&D experts. - R is initially challenging to learn, but Python is linear and simple to understand.\n\n\n1.3.2 Visualizing OLS Linear Regression Assumptions in R\nThis post illustrates some of the assumptions of classical linear regression model which are:\n\nWeak Exogeneity\nLinearity\nConstant Variance\nIndependence of Errors\nLack of Perfect Multicollinearity\n\nLinearity\nLinearity is likely the easiest assumption to visualize as you can simply use the following code snippet to quickly create a scatterplot.\n\n\n\n\nFigure-2.1\n\n\n\nMulticollinearity\nThe convenient way to search for existence of correlation between variables it to plot correlation matrix.\n\n\n\n\nFigure-2.2\n\n\n\nAutocorrelation\nCreating autocorrelation function plot via stats library in R helps detect existence of autocorrelation among error terms statistically.\n\nlibrary(stats)\nmodel <- lm(mpg~drat, data=mtcars)\nacf(model$residuals, type=\"correlation\")\n\n\n\n\n\nFigure-2.3\n\n\n\n\n\n1.3.3 The K-Means Clustering Algorithm Intuition Demonstrated In R\nThis section presents a blogpost by Vincent Tabora demonstrating the K-Means Clustering Algorithm, which is an unsupervised learning method that aims to identify clusters or groups within a given dataset. Each cluster will have a centroid containing the data points or vectors closest to it. It is an iterative process that does not have a predetermined value of how many times to perform.\nSimplified steps for K-Means Clustering are:\n1- Choose the number of clusters K.\n2- Select random K points or the centroids.\n3- Assign the data points to their closest centroids by Euclidean distance.\n4- Reassign data points if they fall closer to another cluster. Compute and place new centroids in the cluster.\n5- Repeat step 3 and 4 until all data points can be grouped in a cluster without re-assigning.\nOptimal Number of Clusters\nA technique called the Elbow Method to find the optimal number of clusters in dataset.\n\nwcss <- vector()\nfor (i in 1:10) wcss[i] <- sum(kmeans(X, i)$withinss)\nplot(1:10, wcss, type = 'b', main = paste('Clusters of Clients'), xlab = 'Number of Clusters', ylab = 'WCSS')\n\nThe rule is to choose the point at which the plot starts decreasing in a linear fashion.\n\n\n\n\nFigure-3.1\n\n\n\nClustering Algorithm\nDataset is into groups using K-Means Clustering. The formula used in K-Means uses the within-cluster sum of squares (wcss) for measuring the compactness of the cluster.\nEach of the observations (Xi) are assigned to a cluster such that the sum of squares (SS) distance of the observations to their assigned cluster centers (μk) are at a minimum.\n\n\n\n\nkmeans <- kmeans(X, 5, iter.max = 300, nstart = 10)\nlibrary(cluster)\nclusplot(X,\n         kmeans$cluster,\n         lines = 0,\n         shade = TRUE,\n         labels = 2,\n         plotchar = FALSE,\n         span = TRUE,\n         main = paste('Clusters of Clients'),\n         xlab = 'Annual Income',\n         ylab = 'Spending Score')\n\nThe result will be to gather insights on the dataset in order to make informed decisions.\nWhat the data shows is that people with high incomes will not always spend higher than everyone else, and there are people with low income who are big spenders.\n\n\n\n\nFigure-3.2"
  },
  {
    "objectID": "In_class_Assignment.html",
    "href": "In_class_Assignment.html",
    "title": "2  In-Class Assignment",
    "section": "",
    "text": "First, start with downloading necessary packages:"
  },
  {
    "objectID": "In_class_Assignment.html#case-1-average-delay",
    "href": "In_class_Assignment.html#case-1-average-delay",
    "title": "2  In-Class Assignment",
    "section": "2.1 Case 1: Average Delay",
    "text": "2.1 Case 1: Average Delay\nAs seen in table below, delays per flight follows a similar pattern among three departure airports in the data:\n\nDelays seem to peak in the summer season.\nDecember also stands out as one of the month when delays rise dramatically as compared to previous month for all three departure airports due to christmas effect.\nWhen considered with number of flights in respective times of the year, although there is no major differences across different months of the year in terms of the number of flights, average delay time per flight changes drastically!.\n\nImportant note for the results below is that, cancelled flights are excluded from calculations as delay information does not exist for those flights in the data table.\n\nflights %>% \n  filter(!is.na(dep_delay)) %>% \n  group_by(month, origin) %>% \n  summarize(count = n(), delay = sum(dep_delay)) %>% \n  mutate(delay_per_flight = delay / count) %>% \n  select(month, origin, count, delay_per_flight) %>% \n  pivot_wider(names_from = origin, values_from = c(delay_per_flight, count))\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 7\n# Groups:   month [12]\n   month delay_per_flight_EWR delay_per_flight…¹ delay…² count…³ count…⁴ count…⁵\n   <int>                <dbl>              <dbl>   <dbl>   <int>   <int>   <int>\n 1     1                14.9                8.62    5.64    9655    9061    7767\n 2     2                13.1               11.8     6.96    8608    8028    7054\n 3     3                18.1               10.7    10.2    10053    9512    8408\n 4     4                17.4               12.2    11.5    10271    9034    8357\n 5     5                15.4               12.5    10.6    10343    9289    8601\n 6     6                22.5               20.5    19.3     9798    9229    8207\n 7     7                22.0               23.8    19.0    10196    9812    8477\n 8     8                13.5               12.9    11.2    10182    9890    8769\n 9     9                 7.29               6.64    6.21    9407    8816    8899\n10    10                 8.64               4.59    5.31   10012    9108    9533\n11    11                 6.72               4.68    4.77    9626    8674    8735\n12    12                21.0               14.8    13.6     9445    8963    8702\n# … with abbreviated variable names ¹​delay_per_flight_JFK,\n#   ²​delay_per_flight_LGA, ³​count_EWR, ⁴​count_JFK, ⁵​count_LGA\n\n\n\n2.1.1 Average delay per flight across different airline companys?\n\ncarrier_delay <- flights %>% \n  filter(!is.na(dep_delay)) %>% \n  group_by(carrier) %>% \n  summarize(count = n(), delay = sum(dep_delay)) %>% \n  transmute(carrier, delay_per_flight = delay / count) %>% \n  arrange(desc(delay_per_flight))\n\ncarrier_delay\n\n# A tibble: 16 × 2\n   carrier delay_per_flight\n   <chr>              <dbl>\n 1 F9                 20.2 \n 2 EV                 20.0 \n 3 YV                 19.0 \n 4 FL                 18.7 \n 5 WN                 17.7 \n 6 9E                 16.7 \n 7 B6                 13.0 \n 8 VX                 12.9 \n 9 OO                 12.6 \n10 UA                 12.1 \n11 MQ                 10.6 \n12 DL                  9.26\n13 AA                  8.59\n14 AS                  5.80\n15 HA                  4.90\n16 US                  3.78"
  },
  {
    "objectID": "In_class_Assignment.html#case-2-average-speed",
    "href": "In_class_Assignment.html#case-2-average-speed",
    "title": "2  In-Class Assignment 1",
    "section": "2.2 Case 2: Average Speed",
    "text": "2.2 Case 2: Average Speed\nAverage speed in miles is calculated per flight using the distance and time spent in the air.\n\n2.2.1 Speed of Flights - Months of the Year\nAverage speed of planes peaks during summer time. Weather conditions (less wind, less friction) is a potential factor.\nOne other factor might be the motivation of pilots to increase their speed and stick to the scheduled arrival time as a result of high delay periods during summer season.\n\nmonth_speed <- flights %>% \n  filter(!is.na(air_time)) %>% \n  transmute(avg_speed = (distance / air_time)*60, month) %>% \n  group_by(month) %>% \n  summarize(mean_speed = mean(avg_speed))\n\nprint(month_speed)\n\n# A tibble: 12 × 2\n   month mean_speed\n   <int>      <dbl>\n 1     1       370.\n 2     2       375.\n 3     3       390.\n 4     4       389.\n 5     5       407.\n 6     6       404.\n 7     7       412.\n 8     8       408.\n 9     9       414.\n10    10       397.\n11    11       385.\n12    12       377.\n\n\n\n\n2.2.2 Speed of Flights - Airline Companies\nThis table shows the average speed of carriers in their flights.\nHawaiian Airlines, Virgin America and Alaska Airline are on top consecutively.\n\ncarrier_speed <- flights %>% \n  filter(!is.na(air_time)&!is.na(dep_delay)) %>% \n  transmute(avg_speed = (distance / air_time)*60, month, carrier) %>% \n  group_by(carrier) %>% \n  summarize(mean = mean(avg_speed)) %>% \n  arrange(desc(mean))\ncarrier_speed\n\n# A tibble: 16 × 2\n   carrier  mean\n   <chr>   <dbl>\n 1 HA       480.\n 2 VX       446.\n 3 AS       444.\n 4 F9       425.\n 5 UA       421.\n 6 DL       418.\n 7 AA       417.\n 8 WN       401.\n 9 B6       400.\n10 FL       394.\n11 MQ       368.\n12 OO       366.\n13 EV       363.\n14 9E       345.\n15 US       342.\n16 YV       332.\n\n\n\n\n2.2.3 Relation Between Speed and Departure Delays\n\ncarrier_joint <- full_join(carrier_delay, carrier_speed,by=\"carrier\")\n\nggplot(carrier_joint, aes(x=delay_per_flight, y = mean)) + \n  geom_point() + geom_smooth(method = \"lm\") + labs(x = \"Average Departure Delay per Flight (minute)\", y = \"Average Speed\", title = \"Speed vs Departure Delay\") + geom_text(aes(label = carrier), nudge_x = 0.5, nudge_y = 0.1, check_overlap = T)"
  },
  {
    "objectID": "In_class_Assignment.html#case-1-average-departure-delay",
    "href": "In_class_Assignment.html#case-1-average-departure-delay",
    "title": "2  In-Class Assignment 1",
    "section": "2.1 Case 1: Average Departure Delay",
    "text": "2.1 Case 1: Average Departure Delay\n\n2.1.1 Average Departure Delay - Months of the Year\nAs seen in table below, delays per flight follows a similar pattern among three departure airports in the data:\n\nDelays seem to peak in the summer season.\nDecember also stands out as one of the month when delays rise dramatically as compared to previous month for all three departure airports due to christmas effect.\nWhen considered with number of flights in respective times of the year, although there is no major differences across different months of the year in terms of the number of flights, average delay time per flight changes drastically!.\n\nImportant note for the results below is that, cancelled flights are excluded from calculations as delay information does not exist for those flights in the data table.\n\nflights %>% \n  filter(!is.na(dep_delay)) %>% \n  group_by(month, origin) %>% \n  summarize(count = n(), delay = sum(dep_delay)) %>% \n  mutate(delay_per_flight = delay / count) %>% \n  select(month, origin, delay_per_flight) %>% \n  pivot_wider(names_from = origin, values_from = delay_per_flight)\n\n# A tibble: 12 × 4\n# Groups:   month [12]\n   month   EWR   JFK   LGA\n   <int> <dbl> <dbl> <dbl>\n 1     1 14.9   8.62  5.64\n 2     2 13.1  11.8   6.96\n 3     3 18.1  10.7  10.2 \n 4     4 17.4  12.2  11.5 \n 5     5 15.4  12.5  10.6 \n 6     6 22.5  20.5  19.3 \n 7     7 22.0  23.8  19.0 \n 8     8 13.5  12.9  11.2 \n 9     9  7.29  6.64  6.21\n10    10  8.64  4.59  5.31\n11    11  6.72  4.68  4.77\n12    12 21.0  14.8  13.6 \n\n\n\n\n2.1.2 Departure Delay - Airline Companies\nFrontier Airlines, Expressjet Airlines and Mesa Airlines are potential late-flyers.\n\ncarrier_delay <- flights %>% \n  filter(!is.na(dep_delay)) %>% \n  group_by(carrier) %>% \n  summarize(count = n(), delay = sum(dep_delay)) %>% \n  transmute(carrier, delay_per_flight = delay / count) %>% \n  arrange(desc(delay_per_flight))\ncarrier_delay\n\n# A tibble: 16 × 2\n   carrier delay_per_flight\n   <chr>              <dbl>\n 1 F9                 20.2 \n 2 EV                 20.0 \n 3 YV                 19.0 \n 4 FL                 18.7 \n 5 WN                 17.7 \n 6 9E                 16.7 \n 7 B6                 13.0 \n 8 VX                 12.9 \n 9 OO                 12.6 \n10 UA                 12.1 \n11 MQ                 10.6 \n12 DL                  9.26\n13 AA                  8.59\n14 AS                  5.80\n15 HA                  4.90\n16 US                  3.78"
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "3  Shiny App",
    "section": "",
    "text": "This section demonstrates a Shiny web application with Data on Foreign Students in Turkish Universities by Their Nationality.\n\n3.0.1 Application\nShiny web application can be accessed via this link\nData processing is accomplished with a few manipulation steps to ready data for further use.\nA web application is then developed plotting a bar chart of top 10 nationalities of foreign students according to the selection of the user regarding city, type and name of the university.\nThe inputs in the application feeds each other in the order of [city > type > name] dynamically.\nBar chart also gives information about gender distribution for each nationality.\n\n\n3.0.2 Command Line for the Shinny Web Application\n\nshiny::runGitHub(\"pjournal/mef06-yigitonay\",subdir=\"Shiny/\", ref = \"gh-pages\")"
  },
  {
    "objectID": "shiny.html#application",
    "href": "shiny.html#application",
    "title": "3  Shiny App",
    "section": "3.1 Application",
    "text": "3.1 Application\nShiny web application can be accessed via this link\nData processing is accomplished with a few manipulation steps to ready data for further use.\nA web application is then developed plotting a bar chart of top 10 nationalities of foreign students according to the selection of the user regarding city, type and name of the university.\nThe inputs in the application feeds each other in the order of [city > type > name] dynamically.\nBar chart also gives information about gender distribution for each nationality."
  },
  {
    "objectID": "shiny.html#command-line-for-the-shinny-web-application",
    "href": "shiny.html#command-line-for-the-shinny-web-application",
    "title": "3  Shiny App",
    "section": "3.2 Command Line for the Shinny Web Application",
    "text": "3.2 Command Line for the Shinny Web Application\n\nshiny::runGitHub(\"pjournal/mef06-yigitonay\",subdir=\"Shiny/\")"
  },
  {
    "objectID": "assignment1.html#introducing-myself",
    "href": "assignment1.html#introducing-myself",
    "title": "1  BDA-503 Data Analytics Essentials: Assignment 1",
    "section": "1.1 Introducing Myself",
    "text": "1.1 Introducing Myself\nHi. This is Yigit. I was born in Ankara, in 1990. I obtained my undergraduate degree from Department of Economics at Bogazici University. After my graduation, I passed the three-stage examination process of Turkiye İş Bankası and started as assistant auditor, which gave me a solid background in banking sector such as the business structure of both headquarters, branches, and their subsidiaries, risk analysis of loan portfolios of corporates and SMEs.\nAs an advancement in my career, I passed the researcher examination of The Central Bank of Republic of Turkey (CBRT) Turkey, and started to work as a researcher. Currently, I am working in Markets Department as an assistant economist in CBRT. My fields of interests consist of macroeconomics, financial modelling and empirical asset pricing in general. To be more specific, most of my studies are about pricing of financial derivatives. Some of my papers with my colleagues are The Determinants of Currency Risk Premium in Emerging Market Countries, A Measure of Turkey’s Sovereign and Banking Sector Credit Risk: Asset Swap Spreads.\nAs we live in a world of massive data, being able to use them efficiently is crucial. The processes of collecting data, analysing them, illustrating the results and preparing automated reports as well as being able to apply modern machine learning techniques to macro-financial data are the main accomplishments I want to make at the end of the Big Data Analytics program.\nYou can reach me via my linkedin page."
  }
]